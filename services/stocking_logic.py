import os
import json
import cv2
import torch
import numpy as np
from PIL import Image
from app.core import ML_CONTEXT


# ================= ğŸ› ï¸ è¾…åŠ©å‡½æ•°ï¼šæ—¶é—´æ ¼å¼åŒ– =================

def format_time(seconds):
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02d}:{m:02d}:{s:02d}" if h > 0 else f"{m:02d}:{s:02d}"


# ================= ğŸ§  æç¤ºè¯åŠ è½½é€»è¾‘ (æ”¯æŒçƒ­æ›´æ–°) =================

def load_prompts_from_file():
    """
    æ¯æ¬¡è¯·æ±‚æ—¶ä» JSON è¯»å–æœ€æ–°çš„ Prompt é…ç½®
    """
    # å‡è®¾ prompts.json åœ¨é¡¹ç›®æ ¹ç›®å½•
    # å¦‚æœä½ åœ¨IDEä¸­è¿è¡Œï¼ŒWorking Directory é€šå¸¸å°±æ˜¯é¡¹ç›®æ ¹ç›®å½•
    json_path = "av_stocking_general.json"

    # ç®€å•çš„é»˜è®¤ç©ºé…ç½®ï¼Œé˜²æ­¢æ–‡ä»¶è¯»å–å¤±è´¥å¯¼è‡´å´©æºƒ
    default_t1 = {"IGNORE": [], "NOISE": [], "TARGET": []}
    default_t2 = {}

    if not os.path.exists(json_path):
        print(f"âš ï¸ [Warning] {json_path} not found. Using empty config.")
        return default_t1, default_t2

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            # ç¡®ä¿è·å–åˆ°å¯¹åº”çš„ key
            t1 = data.get("TIER_1", default_t1)
            t2 = data.get("TIER_2", default_t2)
            return t1, t2
    except Exception as e:
        print(f"âŒ [Error] Failed to read prompts.json: {e}")
        return default_t1, default_t2


def precompute_features():
    """
    åŠ è½½ JSON -> ä½¿ç”¨å…¨å±€æ¨¡å‹è®¡ç®—æ–‡æœ¬ç‰¹å¾
    """
    model = ML_CONTEXT.get('model')
    tokenizer = ML_CONTEXT.get('tokenizer')
    device = ML_CONTEXT.get('device')

    if not model:
        raise RuntimeError("âŒ Model not loaded in ML_CONTEXT")

    # 1. åŠ¨æ€åŠ è½½æç¤ºè¯
    t1_config, t2_config = load_prompts_from_file()

    # --- Tier 1 ç‰¹å¾è®¡ç®— ---
    # å®‰å…¨è·å–åˆ—è¡¨ï¼Œé˜²æ­¢ JSON ç¼ºå°‘ key
    ignores = t1_config.get("IGNORE", [])
    noises = t1_config.get("NOISE", [])
    targets = t1_config.get("TARGET", [])

    t1_prompts = ignores + noises + targets
    c_ign = len(ignores)
    c_noi = len(noises)
    c_tar = len(targets)

    if not t1_prompts:
        # å¦‚æœæç¤ºè¯ä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ dummy feature é˜²æ­¢æŠ¥é”™
        t1_feats = torch.zeros((1, 1024)).to(device)  # å‡è®¾ç»´åº¦
    else:
        with torch.no_grad():
            toks = tokenizer(t1_prompts).to(device)
            t1_feats = model.encode_text(toks)
            t1_feats /= t1_feats.norm(dim=-1, keepdim=True)

    t1_data = {
        "features": t1_feats,
        "slices": {
            "IGNORE": slice(0, c_ign),
            "NOISE": slice(c_ign, c_ign + c_noi),
            "TARGET": slice(c_ign + c_noi, c_ign + c_noi + c_tar)
        }
    }

    # --- Tier 2 ç‰¹å¾è®¡ç®— ---
    t2_data = {}
    for name, config in t2_config.items():
        pos = config.get("pos", [])
        neg = config.get("neg", [])
        weight = config.get("weight", 1.0)  # è·å–æƒé‡ï¼Œé»˜è®¤ä¸º 1.0

        prompts = pos + neg
        if not prompts: continue

        with torch.no_grad():
            toks = tokenizer(prompts).to(device)
            feats = model.encode_text(toks)
            feats /= feats.norm(dim=-1, keepdim=True)

        t2_data[name] = {
            "features": feats,
            "pos_count": len(pos),
            "weight": weight  # å°†æƒé‡å­˜å…¥æ•°æ®åŒ…
        }

    return t1_data, t2_data


# ================= ğŸ¯ æ ¸å¿ƒåˆ†æé€»è¾‘ =================

def analyze_frame_custom(image, t1_data, t2_data):
    """
    å•å¸§åˆ†æå‡½æ•°
    """
    model = ML_CONTEXT['model']
    preprocess = ML_CONTEXT['preprocess']
    device = ML_CONTEXT['device']

    # å›¾åƒé¢„å¤„ç†
    image_input = preprocess(image).unsqueeze(0).to(device)

    with torch.no_grad():
        img_features = model.encode_image(image_input)
        img_features /= img_features.norm(dim=-1, keepdim=True)

        # === 1. Tier 1: ç²—ç­› ===
        # å³ä½¿ t1_prompts ä¸ºç©ºï¼Œè¿™é‡Œä¹Ÿä¼šç®—å‡º meaningless scoresï¼Œä¸ä¼šå´©
        if t1_data["features"].shape[0] > 1:
            raw_scores = (100.0 * img_features @ t1_data["features"].T).cpu().numpy()[0]

            def get_group_score(slice_obj):
                if slice_obj.start == slice_obj.stop: return 0.0
                scores = raw_scores[slice_obj]
                scores.sort()
                top_k = scores[-3:] if len(scores) >= 3 else scores
                return np.mean(top_k)

            s_ignore = get_group_score(t1_data["slices"]["IGNORE"])
            s_noise = get_group_score(t1_data["slices"]["NOISE"])
            s_target = get_group_score(t1_data["slices"]["TARGET"])

            # é—¨å«åˆ¤å®šé€»è¾‘
            if (s_target < s_ignore) or (s_target < s_noise) or (s_target < 22.0):
                return "IGNORE", float(max(s_ignore, s_noise))
        else:
            # å¦‚æœæ²¡æœ‰é…ç½® Tier 1ï¼Œé»˜è®¤è·³è¿‡é—¨å«ï¼ˆæˆ–è€…é»˜è®¤æ‹¦æˆªï¼Œå–å†³äºä½ çš„éœ€æ±‚ï¼Œè¿™é‡Œé»˜è®¤æ‹¦æˆªï¼‰
            s_target = 0.0

        # === 2. Tier 2: ç»†åˆ†ä¸æƒé‡åº”ç”¨ ===
        cat_scores = {}
        for name, data in t2_data.items():
            probs = (100.0 * img_features @ data["features"].T).softmax(dim=-1).cpu().numpy()[0]
            raw_score = float(sum(probs[:data["pos_count"]]))

            # ğŸ”¥ æ ¸å¿ƒï¼šåº”ç”¨ JSON ä¸­é…ç½®çš„ weight
            weight = data["weight"]

            # åªæœ‰å½“åŸå§‹åˆ†è¾¾åˆ°ä¸€å®šåŸºå‡† (0.25) ä¸”æƒé‡æœ‰æ•ˆæ—¶ï¼Œæ‰è¿›è¡ŒåŠ æƒ
            if raw_score > 0.25 and weight != 1.0:
                final_score = raw_score * weight
            else:
                final_score = raw_score

            cat_scores[name] = final_score

        if not cat_scores:
            return "others", float(s_target / 100.0)

        # é€‰å‡ºæœ€é«˜åˆ†
        best_label = max(cat_scores, key=cat_scores.get)
        best_score = cat_scores[best_label]

        if best_score > 0.35:
            return best_label, best_score

        # å…œåº•
        return "others", float(s_target / 100.0)


# ================= âœ‚ï¸ æ—¶é—´è½´åˆå¹¶é€»è¾‘ =================

# ================= âœ‚ï¸ ä¼˜åŒ–åçš„æ—¶é—´è½´åˆå¹¶é€»è¾‘ =================

def merge_timeline(raw_events):
    """
    ä¸¤é˜¶æ®µåˆå¹¶ç­–ç•¥ï¼šé˜²æ­¢åˆ‡åˆ†å¤ªç¢
    """
    if not raw_events: return []
    # æŒ‰æ—¶é—´æ’åº
    raw_events.sort(key=lambda x: x[0])

    # --- ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç‰©ç†åˆå¹¶ (åŒæ ‡ç­¾ï¼Œæ—¶é—´è¿ç»­) ---
    # ç­–ç•¥ï¼šåªè¦é—´éš”å°äº 5ç§’ ä¸” æ ‡ç­¾ç›¸åŒï¼Œå°±è§†ä¸ºç‰©ç†è¿ç»­
    BASE_TOLERANCE = 5

    pass1_segments = []
    curr_label = None
    start_t = 0
    last_t = 0
    score_accum = 0
    count = 0

    for t, label, score in raw_events:
        if curr_label is None:
            curr_label = label
            start_t = t
            last_t = t
            score_accum = score
            count = 1
            continue

        time_gap = t - last_t

        # åªè¦æ˜¯åŒä¸€ä¸ªæ ‡ç­¾ï¼Œä¸”ä¸­é—´æ–­æ¡£å¾ˆå°ï¼Œå°±ç›´æ¥è¿èµ·æ¥
        if (label == curr_label) and (time_gap <= BASE_TOLERANCE):
            last_t = t
            score_accum += score
            count += 1
        else:
            # ç»“ç®—
            pass1_segments.append({
                "start": start_t,
                "end": last_t,
                "duration": last_t - start_t,
                "label": curr_label,
                "score": score_accum / count
            })
            # å¼€å¯æ–°æ®µ
            curr_label = label
            start_t = t
            last_t = t
            score_accum = score
            count = 1

    # ç»“ç®—æœ€åä¸€æ®µ
    if curr_label:
        pass1_segments.append({
            "start": start_t,
            "end": last_t,
            "duration": last_t - start_t,
            "label": curr_label,
            "score": score_accum / count
        })

    # --- ç¬¬äºŒé˜¶æ®µï¼šè¯­ä¹‰ç¼åˆ (Semantic Stitching) ---
    # è§£å†³ "ç¢" çš„æ ¸å¿ƒï¼šå¦‚æœä¸¤æ®µ "ä¸è¢œ" ä¸­é—´éš”äº† 30ç§’ çš„ "others" æˆ– "ç©ºçª—"ï¼Œ
    # æˆ‘ä»¬è®¤ä¸ºè¿™å…¶å®æ˜¯åŒä¸€åœºæˆï¼Œå¼ºè¡Œåˆå¹¶ã€‚

    SEMANTIC_GAP_TOLERANCE = 60.0  # ğŸ”¥ æ ¸å¿ƒå‚æ•°ï¼šå…è®¸æœ€å¤§ 60ç§’ çš„è·¨åº¦
    MIN_FINAL_DURATION = 8.0  # ğŸ”¥ æœ€ç»ˆè¿‡æ»¤ï¼šå°äº 8ç§’ çš„ç‰‡æ®µä¸è¦

    final_segments = []
    if not pass1_segments: return []

    # å–å‡ºç¬¬ä¸€ä¸ªç‰‡æ®µä½œä¸ºå½“å‰åŸºå‡†
    current_seg = pass1_segments[0]

    for next_seg in pass1_segments[1:]:
        # è®¡ç®—ä¸¤ä¸ªç‰‡æ®µä¹‹é—´çš„ç©ºéš™
        gap = next_seg["start"] - current_seg["end"]

        is_same_label = (current_seg["label"] == next_seg["label"])
        is_close_enough = (gap <= SEMANTIC_GAP_TOLERANCE)

        if is_same_label and is_close_enough:
            # âœ… æ‰§è¡Œåˆå¹¶
            # æ›´æ–°ç»“æŸæ—¶é—´
            current_seg["end"] = next_seg["end"]
            # æ›´æ–°æ—¶é•¿
            current_seg["duration"] = current_seg["end"] - current_seg["start"]
            # æ›´æ–°åˆ†æ•° (åŠ æƒå¹³å‡ï¼Œç®€å•å¤„ç†å°±å–å¹³å‡)
            current_seg["score"] = (current_seg["score"] + next_seg["score"]) / 2
        else:
            # âŒ æ— æ³•åˆå¹¶ï¼Œå°†å½“å‰ç‰‡æ®µå½’æ¡£
            if current_seg["duration"] >= MIN_FINAL_DURATION:
                current_seg["time_str"] = f"{format_time(current_seg['start'])} - {format_time(current_seg['end'])}"
                final_segments.append(current_seg)

            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
            current_seg = next_seg

    # åˆ«å¿˜äº†æœ€åä¸€ä¸ª
    if current_seg["duration"] >= MIN_FINAL_DURATION:
        current_seg["time_str"] = f"{format_time(current_seg['start'])} - {format_time(current_seg['end'])}"
        final_segments.append(current_seg)

    return final_segments


# ================= ğŸ¬ ä¸šåŠ¡ä¸»å…¥å£ =================

def execute_stocking_scan(video_path: str):
    """
    ä¸šåŠ¡å…¥å£ï¼šè¯»å– JSON -> é¢„è®¡ç®—ç‰¹å¾ -> æ‰«æè§†é¢‘ -> åˆå¹¶ç»“æœ
    """
    print(f"\nğŸ¬ [StockingLogic] Start scanning: {os.path.basename(video_path)}")

    # 1. é¢„è®¡ç®—ç‰¹å¾ (æ¯æ¬¡è¯·æ±‚éƒ½ä¼šæ‰§è¡Œï¼Œä¿è¯ JSON å˜åŠ¨å³æ—¶ç”Ÿæ•ˆ)
    try:
        t1_data, t2_data = precompute_features()
    except Exception as e:
        print(f"âŒ Feature computation failed: {e}")
        raise e

    # 2. è§†é¢‘åˆå§‹åŒ–
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"Cannot open video: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS) or 24.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # ç¡¬ç¼–ç ç­–ç•¥ï¼š2ç§’æ£€æµ‹ä¸€æ¬¡
    CHECK_INTERVAL = 2
    step_frames = int(fps * CHECK_INTERVAL)

    raw_timeline = []

    try:
        # ä½¿ç”¨ OpenCV éå†
        for frame_idx in range(0, total_frames, step_frames):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret: break

            current_sec = frame_idx / fps

            # BGR -> RGB -> PIL
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame_rgb)

            # æ ¸å¿ƒè¯†åˆ«
            label, score = analyze_frame_custom(pil_img, t1_data, t2_data)

            if label != "IGNORE":
                raw_timeline.append((current_sec, label, score))
                # å®æ—¶æ—¥å¿— (å¯é€‰ï¼Œç”Ÿäº§ç¯å¢ƒå¯æ³¨é‡Š)
                # print(f"\rFound: {label} at {format_time(current_sec)} ({score:.2f})", end="")

    finally:
        cap.release()

    print(f"\nâœ… [StockingLogic] Scan finished. Merging timeline...")

    # 3. ç»“æœåˆå¹¶
    final_segments = merge_timeline(raw_timeline)

    return final_segments